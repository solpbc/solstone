# solstone Journal Guide

This document describes the layout of a **journal** directory where all captures, extracts, and insights are stored. Each dated `YYYYMMDD` folder is referred to as a **day**, and within each day captured content is organized into **segments** (timestamped duration folders). Each segment folder uses the format `HHMMSS_LEN/` where `HHMMSS` is the start time and `LEN` is the duration in seconds. This folder name serves as the **segment key**, uniquely identifying the segment within a given day.

## The Three-Layer Architecture

solstone transforms raw recordings into actionable understanding through a three-layer pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 3: INSIGHTS                  â”‚  Narrative summaries
â”‚  (Markdown files)                   â”‚  "What it means"
â”‚  - insights/*.md (daily insights)   â”‚
â”‚  - *.md (segment insights)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘ synthesized from
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 2: EXTRACTS                  â”‚  Structured data
â”‚  (JSON/JSONL files)                 â”‚  "What happened"
â”‚  - audio.jsonl, *_audio.jsonl       â”‚
â”‚  - screen.jsonl, *_screen.jsonl     â”‚
â”‚  - events/*.jsonl (per-facet)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘ derived from
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 1: CAPTURES                  â”‚  Raw recordings
â”‚  (Binary media files)               â”‚  "What was recorded"
â”‚  - *.flac (audio)                   â”‚
â”‚  - *.webm (video)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Vocabulary Quick Reference

**Pipeline Layers**

| Term | Definition | Examples |
|------|------------|----------|
| **Capture** | Raw audio/video recording | `*.flac`, `*.webm` |
| **Extract** | Structured data from captures | `*.jsonl` |
| **Insight** | AI-generated narrative summary | `insights/*.md`, `HHMMSS_LEN/*.md` |

**Organization**

| Term | Definition | Examples |
|------|------------|----------|
| **Day** | 24-hour activity directory | `20250119/` |
| **Segment** | 5-minute time window | `143022_300/` (14:30:22, 5 min) |
| **Facet** | Project/context scope | `#work`, `#personal` |

**Extracted Data**

| Term | Definition | Examples |
|------|------------|----------|
| **Entity** | Tracked person/project/concept | People, companies, tools |
| **Occurrence** | Time-based event | Meetings, messages, files |

## Top-Level Directory Structure

| Directory/File | Purpose |
|----------------|---------|
| `YYYYMMDD/` | Daily capture folders containing segments, extracts, and insights |
| `facets/` | Facet-specific data: entities, todos, events, news, action logs |
| `agents/` | Agent event logs (`<id>.jsonl`, `<id>_active.jsonl` for running agents) |
| `apps/` | App-specific storage (distinct from codebase `apps/`) |
| `imports/` | Imported audio files and processing artifacts |
| `tokens/` | Token usage logs from AI model calls, organized by day |
| `indexer/` | Search index (`journal.sqlite` FTS5 database) |
| `health/` | Service health logs (`<service>.log` files) |
| `config/` | Configuration files and journal-level action logs |
| `task_log.txt` | Optional log of utility runs in `[epoch]\tmessage` format |
| `summary.md` | Journal-wide statistics summary (generated by `sol journal-stats`) |
| `stats.json` | Detailed journal statistics in JSON format (generated by `sol journal-stats`) |

### Config directory

- `config/journal.json` â€“ user configuration for the journal (optional, see [User configuration](#user-configuration)).
- `config/convey.json` â€“ Convey UI preferences (facet/app ordering, selected facet).
- `config/actions/` â€“ journal-level action logs (see [Action Logs](#action-logs)).

## User configuration

The optional `config/journal.json` file allows customization of journal processing and presentation based on user preferences. This file should be created at the journal root and contains personal settings that affect how the system processes and interprets journal data.

### Identity configuration

The `identity` block contains information about the journal owner that helps tools correctly identify the user in transcripts, meetings, and other captured content:

```json
{
  "identity": {
    "name": "Jeremie Miller",
    "preferred": "Jer",
    "pronouns": {
      "subject": "he",
      "object": "him",
      "possessive": "his",
      "reflexive": "himself"
    },
    "aliases": ["Jer", "jeremie"],
    "email_addresses": ["jer@example.com"],
    "timezone": "America/Los_Angeles"
  }
}
```

Fields:
- `name` (string) â€“ Full legal or formal name of the journal owner
- `preferred` (string) â€“ Preferred name or nickname to be used when addressing the user
- `pronouns` (object) â€“ Structured pronoun set for template usage with fields:
  - `subject` â€“ Subject pronoun (e.g., "he", "she", "they")
  - `object` â€“ Object pronoun (e.g., "him", "her", "them")
  - `possessive` â€“ Possessive adjective (e.g., "his", "her", "their")
  - `reflexive` â€“ Reflexive pronoun (e.g., "himself", "herself", "themselves")
- `aliases` (array of strings) â€“ Alternative names, nicknames, or usernames that may appear in transcripts
- `email_addresses` (array of strings) â€“ Email addresses associated with the user for participant detection
- `timezone` (string) â€“ IANA timezone identifier (e.g., "America/New_York", "Europe/London") for timestamp interpretation

This configuration helps meeting extraction identify the user as a participant, enables personalized agent interactions, and ensures timestamps are interpreted correctly across the journal.

### Convey configuration

The `convey` block contains settings for the web application:

```json
{
  "convey": {
    "password": "your-password-here"
  }
}
```

Fields:
- `password` (string) â€“ Password for accessing the convey web application. When set, users must authenticate before accessing the journal interface.

**UI Preferences:** The separate `config/convey.json` file stores UI/UX personalization (facet/app ordering, selected facet). All fields optional:

```json
{
  "facets": {"order": ["work", "personal"], "selected": "work"},
  "apps": {"order": ["home", "calendar", "todos"], "starred": ["home", "todos"]}
}
```

- `facets.order` â€“ Custom facet ordering. `facets.selected` â€“ Currently selected facet (auto-synced with browser).
- `apps.order` â€“ Custom app ordering in menu bar.
- `apps.starred` â€“ Apps to show in the quick-access starred section.

### Environment variables

The `env` block provides fallback values for environment variables. These are loaded at CLI startup and used when the corresponding variable is not set in the shell or `.env` file:

```json
{
  "env": {
    "GOOGLE_API_KEY": "your-google-api-key",
    "ANTHROPIC_API_KEY": "your-anthropic-api-key",
    "OPENAI_API_KEY": "your-openai-api-key",
    "REVAI_ACCESS_TOKEN": "your-revai-token"
  }
}
```

**Precedence order** (highest to lowest):
1. Shell environment variables
2. `.env` file in project root
3. Journal config `env` section

This allows storing API keys in the journal config as an alternative to `.env`, which can be useful when the journal is synced across machines or when you want to keep all configuration in one place.

#### Template usage examples

The structured pronoun format enables proper pronoun usage in generated text and agent responses:

```python
# In templates or generated text:
f"{identity.pronouns.subject} joined the meeting"  # "he joined the meeting"
f"I spoke with {identity.pronouns.object}"         # "I spoke with him"
f"That is {identity.pronouns.possessive} desk"     # "That is his desk"
f"{identity.pronouns.subject} did it {identity.pronouns.reflexive}"  # "he did it himself"
```

### Transcribe configuration

The `transcribe` block configures audio transcription settings for `sol transcribe`:

```json
{
  "transcribe": {
    "backend": "whisper",
    "enrich": true,
    "preserve_all": false,
    "whisper": {
      "device": "auto",
      "model": "medium.en",
      "compute_type": "default"
    },
    "revai": {
      "model": "fusion"
    }
  }
}
```

**Top-level fields:**
- `backend` (string) â€“ STT backend to use: `"whisper"` (local processing) or `"revai"` (cloud with speaker diarization). Default: `"whisper"`.
- `enrich` (boolean) â€“ Enable LLM enrichment for topic extraction and transcript correction. Default: `true`.
- `preserve_all` (boolean) â€“ Keep audio files even when no speech is detected. When `false`, silent recordings are deleted to save disk space. Default: `false`.

**Whisper backend settings** (`transcribe.whisper`):
- `device` (string) â€“ Device for inference: `"auto"` (detect GPU, fall back to CPU), `"cpu"`, or `"cuda"`. Default: `"auto"`.
- `model` (string) â€“ Whisper model to use (e.g., `"tiny.en"`, `"base.en"`, `"small.en"`, `"medium.en"`, `"large-v3-turbo"`, `"distil-large-v3"`). Default: `"medium.en"`.
- `compute_type` (string) â€“ Compute precision: `"default"` (auto-select optimal for platform), `"float32"` (most compatible), `"float16"` (faster on CUDA GPUs), `"int8"` (fastest on CPU). Default: `"default"`.

**Rev.ai backend settings** (`transcribe.revai`):
- `model` (string) â€“ Rev.ai transcriber model: `"fusion"` (best quality), `"machine"` (fast automated), or `"low_cost"`. Default: `"fusion"`.

**Platform auto-detection** (Whisper): When `compute_type` is `"default"`, optimal settings are automatically selected:
- **CUDA GPU**: Uses `float16` for GPU-optimized inference
- **CPU (including Apple Silicon)**: Uses `int8` for ~2x faster inference and significantly faster model loading

Voice embeddings (resemblyzer) also auto-detect the best device: MPS on Apple Silicon (~16x faster), CUDA when available, or CPU fallback.

CLI flags can override settings: `--backend` selects the backend, `--cpu` forces CPU mode with int8 (Whisper only), `--model MODEL` overrides the Whisper model.

### Describe configuration

The `describe` block configures screen analysis settings for `sol describe`:

```json
{
  "describe": {
    "max_extractions": 20,
    "categories": {
      "code": {
        "importance": "high",
        "extraction": "Extract when viewing different repositories or files"
      },
      "gaming": {
        "importance": "ignore"
      }
    }
  }
}
```

**Fields:**
- `max_extractions` (integer) â€“ Maximum number of frames to run detailed content extraction on per video. The first qualified frame is always extracted regardless of this limit. When more frames are eligible, selection uses AI-based prioritization (falling back to random selection). Default: `20`.
- `categories` (object) â€“ Per-category overrides for importance and extraction guidance.

#### Category overrides

Each category (e.g., `code`, `meeting`, `browsing`) can have:

| Field | Values | Description |
|-------|--------|-------------|
| `importance` | `high`, `normal`, `low`, `ignore` | Advisory priority hint for AI frame selection. `high` prioritizes these frames, `low` deprioritizes unless unique, `ignore` suggests skipping unless categorization seems wrong. Default: `normal`. |
| `extraction` | string | Custom guidance for when to extract content from this category. Overrides the default from the category's `.json` file. |

Importance levels are advisory hints passed to the AI selection process, not hard filters. The AI may still select frames from `ignore` categories if it determines the content is valuable or the categorization may be incorrect.

### Providers configuration

The `providers` block enables fine-grained control over which LLM provider and model is used for different contexts. This supports a tier-based system where you can specify capability levels (pro/flash/lite) rather than specific model names.

```json
{
  "providers": {
    "default": {
      "provider": "google",
      "tier": 2
    },
    "contexts": {
      "observe.*": {"provider": "google", "tier": 3},
      "insight.*": {"tier": 1},
      "agent.helper": {"provider": "openai", "model": "gpt-5-mini"}
    },
    "models": {
      "google": {
        "1": "gemini-3-pro-preview",
        "2": "gemini-3-flash-preview",
        "3": "gemini-2.5-flash-lite"
      }
    }
  }
}
```

#### Tier system

Tiers provide a provider-agnostic way to specify model capability levels:

| Tier | Name  | Description |
|------|-------|-------------|
| 1    | pro   | Highest capability, best for complex reasoning |
| 2    | flash | Balanced performance and cost (default) |
| 3    | lite  | Fastest and cheapest, for simple tasks |

System defaults map tiers to models for each provider. See `muse/models.py` for current tier-to-model mappings (`PROVIDER_DEFAULTS` constant).

If a requested tier is unavailable for a provider, the system falls back to more capable tiers (e.g., tier 3 â†’ tier 2 â†’ tier 1).

#### Context matching

Contexts are matched in order of specificity:
1. **Exact match** â€“ `"insight.meetings"` matches only that exact context
2. **Glob pattern** â€“ `"observe.*"` matches any context starting with `observe.`
3. **Default** â€“ Falls back to the `default` configuration

#### Configuration options

**default** â€“ Global defaults applied when no context matches:
- `provider` (string) â€“ Provider name: `"google"`, `"openai"`, or `"anthropic"`. Default: `"google"`.
- `tier` (integer) â€“ Tier number (1-3). Default: `2` (flash).
- `model` (string) â€“ Explicit model name (overrides tier if specified).

**contexts** â€“ Context-specific overrides. Each key is a context pattern, value is:
- `provider` (string) â€“ Override provider (optional, inherits from default).
- `tier` (integer) â€“ Tier number (optional).
- `model` (string) â€“ Explicit model name (optional, overrides tier).

**models** â€“ Per-provider tier overrides. Maps provider name to tier-model mappings:
```json
{
  "google": {"1": "gemini-3-pro-preview", "2": "gemini-3-flash-preview"},
  "openai": {"2": "gpt-5-mini-custom"}
}
```

Note: Tier keys in JSON must be strings (`"1"`, `"2"`, `"3"`) since JSON doesn't support integer keys.

## Facet folders

The `facets/` directory provides a way to organize journal content by scope or focus area. Each facet represents a cohesive grouping of related activities, projects, or areas of interest.

### Facet structure

Each facet is organized as `facets/<facet>/` where `<facet>` is a descriptive short unique name. When referencing facets in the system, use hashtags (e.g., `#personal` for the "Personal Life" facet, `#ml_research` for "Machine Learning Research"). Each facet folder contains:

- `facet.json` â€“ metadata file with facet title and description.
- `entities.jsonl` â€“ entities specific to this facet in JSONL format.
- `entities/` â€“ daily detected entities (see [Facet Entities](#facet-entities)).
- `todos/` â€“ daily todo lists (see [Facet-Scoped Todos](#facet-scoped-todos)).
- `events/` â€“ extracted events per day (see [Event extracts](#event-extracts)).
- `news/` â€“ daily news and updates relevant to the facet (optional).
- `logs/` â€“ action audit logs for MCP tool calls (optional, see [Action Logs](#action-logs)).

### Facet metadata

The `facet.json` file contains basic information about the facet:

```json
{
  "title": "Machine Learning Research",
  "description": "AI/ML research projects, experiments, and related activities",
  "color": "#4f46e5",
  "emoji": "ğŸ§ "
}
```

Optional fields:
- `color` â€“ hex color code for the facet card background in the web UI
- `emoji` â€“ emoji icon displayed in the top-left of the facet card
- `muted` â€“ boolean flag to mute/hide the facet from views (default: false)

### Facet Entities

Entities in solstone use a two-state system: **detected** (daily discoveries) and **attached** (promoted/persistent). This agent-driven architecture automatically identifies entities from journal content while allowing manual curation.

#### Entity Storage Structure

```
facets/{facet}/
  â”œâ”€â”€ entities.jsonl              # Attached entities (persistent)
  â””â”€â”€ entities/
      â”œâ”€â”€ YYYYMMDD.jsonl          # Daily detected entities
      â””â”€â”€ {normalized_name}/      # Entity memory folder (optional)
```

**Entity memory folders** store persistent data the system "remembers" about attached entitiesâ€”observations (durable facts), voiceprints (voice recognition), and profile images. Folders are created on-demand when memory is added. The folder name is the entity name normalized to lowercase with underscores (e.g., "Alice Johnson" â†’ `alice_johnson/`). Folders are renamed automatically when entities are renamed.

#### Attached Entities

The `entities.jsonl` file contains manually promoted entities that are persistently associated with the facet. These entities are loaded into agent context and appear in the facet UI as starred items.

**Entity names must be unique within a facet** (regardless of type). The `id` field provides a stable slug identifier for programmatic references.

Format example (JSONL - one JSON object per line):
```jsonl
{"id": "alice_johnson", "type": "Person", "name": "Alice Johnson", "description": "Lead engineer on the API project", "aka": ["Ali", "AJ"]}
{"id": "techcorp", "type": "Company", "name": "TechCorp", "description": "Primary client for consulting work", "tier": "enterprise", "aka": ["TC", "TechCo"]}
{"id": "api_optimization", "type": "Project", "name": "API Optimization", "description": "Performance improvement initiative", "status": "active", "priority": "high"}
{"id": "postgresql", "type": "Tool", "name": "PostgreSQL", "description": "Database system used in production", "version": "16.0", "aka": ["Postgres", "PG"]}
```

Entity types are flexible and user-defined. Common examples: `Person`, `Company`, `Project`, `Tool`, `Location`, `Event`. Type names must be alphanumeric with spaces, minimum 3 characters.

Each entity is a JSON object with required fields (`id`, `type`, `name`, `description`) and optional custom fields for extensibility (e.g., `status`, `priority`, `tags`, `contact`, etc.). Custom fields are preserved throughout the system.

**Standard fields:**
- `id` (string) â€“ Stable slug identifier derived from name via `entity_slug()` in `think/entities.py` (lowercase, spaces replaced with underscores, e.g., "Alice Johnson" â†’ "alice_johnson"). Used for folder paths, URLs, and MCP tool references. Automatically regenerated when name changes.
- `aka` (array of strings) â€“ Alternative names, nicknames, or acronyms for the entity. Used in audio transcription to improve entity recognition.
- `detached` (boolean) â€“ When `true`, marks the entity as soft-deleted. Detached entities remain in the file but are hidden from UI and excluded from agent context. This preserves entity history and allows re-attachment without data loss.
- `attached_at` (integer) â€“ Unix timestamp in milliseconds when entity was first attached.
- `updated_at` (integer) â€“ Unix timestamp in milliseconds of last modification.
- `last_seen` (string) â€“ Day in YYYYMMDD format when entity was last mentioned in journal content. Automatically updated after daily processing by parsing the knowledge graph and matching entity names via fuzzy matching.

#### Detected Entities

Daily entity detection files (`entities/YYYYMMDD.jsonl`) contain entities automatically discovered by agents from:
- Journal transcripts and screen captures
- Knowledge graphs and insights
- News feeds and external content

Detected entities accumulate historical context over time. Entities appearing in multiple daily detections can be promoted to attached status through the web UI or MCP tools.

Format matches attached entities (JSONL):
```jsonl
{"type": "Person", "name": "Charlie Brown", "description": "Mentioned in standup meeting"}
{"type": "Tool", "name": "React", "description": "Used in UI development work"}
```

#### Entity Lifecycle

1. **Detection**: Daily agents scan journal content and record entities in `entities/YYYYMMDD.jsonl`
2. **Aggregation**: Review agent tracks detection frequency across recent days
3. **Promotion**: Entities with 3+ detections are auto-promoted to attached, or users manually promote via UI
4. **Persistence**: Attached entities in `entities.jsonl` remain active until detached
5. **Detachment**: When removed via UI, entities are soft-deleted (`detached: true`) preserving all metadata
6. **Re-attachment**: Detached entities can be re-activated, restoring them with preserved history (original `attached_at`, updated `updated_at`)

#### Cross-Facet Behavior

The same entity name can exist in multiple facets with independent descriptions. Agents receive entity context from all facets, with alphabetically-first facet winning for name conflicts during aggregation.

### Facet News

The `news/` directory provides a chronological record of news, updates, and external developments relevant to the facet. This allows tracking of industry news, research updates, regulatory changes, or any external information that impacts the facet's focus area.

#### News organization

News files are organized by date as `news/YYYYMMDD.md` where each file contains the day's relevant news items. Only create files for days that have news to recordâ€”sparse population is expected.

#### News file format

Each `YYYYMMDD.md` file is a markdown document with a consistent structure:

```markdown
# 2025-01-18 News - Machine Learning Research

## OpenAI Announces New Model Architecture
**Source:** techcrunch.com | **Time:** 09:15
Summary of the announcement and its relevance to current research projects...

## Paper: "Efficient Attention Mechanisms in Transformers"
**Source:** arxiv.org | **Time:** 14:30
Key findings from the paper and potential applications...

## Google Research Updates Dataset License Terms
**Source:** blog.google | **Time:** 16:45
Changes to dataset licensing that may affect ongoing experiments...
```

#### News entry structure

Each news entry should include:
- **Title** â€“ concise headline as a level 2 heading
- **Source** â€“ origin of the news (website, journal, etc.)
- **Time** â€“ optional time of publication or discovery (HH:MM format)
- **Summary** â€“ brief description focusing on relevance to the facet
- **Impact** â€“ optional notes on how this affects facet work

#### News metadata

Optionally, a `news.json` file can be maintained at the root of the news directory to track metadata:

```json
{
  "last_updated": "2025-01-18",
  "sources": ["arxiv.org", "techcrunch.com", "nature.com"],
  "auto_fetch": false,
  "keywords": ["transformer", "attention", "llm", "research"]
}
```

This allows for future automation of news gathering while maintaining manual curation quality.

## Facet-Scoped Todos

Todos are organized by facet in `facets/{facet}/todos/{day}.jsonl` where each file stores todo items as JSON Lines. Todos belong to a specific facet (e.g., "personal", "work", "research") and are completely separated by scope.

**File path pattern:**
```
facets/personal/todos/20250110.jsonl
facets/work/todos/20250110.jsonl
facets/research/todos/20250112.jsonl
```

Each file contains one JSON object per line, with the line number (1-indexed) serving as the stable todo ID.

```jsonl
{"text": "Draft standup update"}
{"text": "Review PR #1234 for indexing tweaks", "time": "14:30"}
{"text": "Morning planning session notes", "completed": true}
{"text": "Cancel meeting with vendor", "cancelled": true}
```

### Format Specification

**JSONL structure:**

Each line is a JSON object with the following fields:
- `text` (required) â€“ Task description
- `time` (optional) â€“ Scheduled time in `HH:MM` format (e.g., `"14:30"`)
- `completed` (optional) â€“ Set to `true` when task is done
- `cancelled` (optional) â€“ Set to `true` for soft-deleted tasks
- `created_at` (optional) â€“ Unix timestamp in milliseconds when todo was created
- `updated_at` (optional) â€“ Unix timestamp in milliseconds of last modification

**Facet context:**
- Facet is determined by the file location, not inline tags
- Each facet has its own independent todo list for each day
- Work todos (`facets/work/todos/`) are completely separate from personal todos (`facets/personal/todos/`)

**Rules:**
- Line number is the stable todo ID (1-indexed); todos are never removed, only cancelled
- Append new todos at the end of the file to maintain stable line numbering
- Mark completed items with `"completed": true`
- Cancel items with `"cancelled": true` (soft delete preserves line numbers)

**MCP Tool Access:**
All todo operations require both `day` and `facet` parameters:
- `todo_list(day, facet)` â€“ view numbered checklist for a specific facet
- `todo_add(day, facet, line_number, text)` â€“ add new todo (line_number must match next available)
- `todo_done(day, facet, line_number)` â€“ mark complete
- `todo_cancel(day, facet, line_number)` â€“ cancel entry (soft delete)
- `todo_upcoming(limit, facet=None)` â€“ view upcoming todos (optionally filtered by facet)

This facet-scoped structure provides true separation of concerns while enabling automated tools to manage tasks deterministically.

## Action Logs

Action logs record an audit trail of user-initiated actions and MCP tool calls. There are two types:

- **Journal-level logs** (`config/actions/`) â€“ actions not tied to a specific facet (settings changes, remote observer management)
- **Facet-scoped logs** (`facets/{facet}/logs/`) â€“ actions within a specific facet (todos, entities)

### Journal Action Logs

The `config/actions/` directory records journal-level actions. Logs are organized by day as `config/actions/YYYYMMDD.jsonl`.

```json
{
  "timestamp": "2025-12-16T07:33:05.135587+00:00",
  "source": "app",
  "actor": "settings",
  "action": "identity_update",
  "params": {
    "changed_fields": {"name": {"old": "John", "new": "John Doe"}}
  }
}
```

### Facet Action Logs

The `logs/` directory within each facet records facet-scoped actions. Logs are organized by day as `facets/{facet}/logs/YYYYMMDD.jsonl`.

```json
{
  "timestamp": "2025-12-16T07:33:05.135587+00:00",
  "source": "tool",
  "actor": "todos:todo",
  "action": "todo_add",
  "params": {
    "line_number": 1,
    "text": "Review project proposal"
  },
  "facet": "work",
  "agent_id": "1765870373972"
}
```

### Log Entry Fields

Both log types share the same structure:

- `timestamp` â€“ ISO 8601 timestamp of the action
- `source` â€“ Origin type: "app" for web UI, "tool" for MCP tools
- `actor` â€“ App or tool name that performed the action
- `action` â€“ Action name (e.g., "todo_add", "identity_update")
- `params` â€“ Action-specific parameters
- `facet` â€“ Facet name (only present in facet-scoped logs)
- `agent_id` â€“ Agent ID (only present for MCP tool actions)

These logs enable auditing, debugging, and potential rollback of automated actions.

## Token Usage

The `tokens/` directory tracks token usage from all AI model calls across the system. Usage data is organized by day as `tokens/YYYYMMDD.jsonl` where each file contains JSON Lines entries for that day's API calls.

### Token log format

Each line in a token log file is a JSON object with the following structure:

```json
{
  "timestamp": 1736812345000,
  "model": "gemini-2.5-flash",
  "context": "agent.default.20250113_143022",
  "segment": "143022_300",
  "usage": {
    "input_tokens": 1500,
    "output_tokens": 500,
    "total_tokens": 2000,
    "cached_tokens": 800,
    "reasoning_tokens": 200
  }
}
```

Required fields:
- `timestamp` â€“ Unix timestamp in milliseconds (13 digits)
- `model` â€“ Model identifier (e.g., "gemini-2.5-flash", "gpt-5", "claude-sonnet-4-5")
- `context` â€“ Calling context (e.g., "agent.persona.agent_id" or "module.function:line")
- `usage` â€“ Token counts dictionary with normalized field names

Optional fields:
- `segment` â€“ Recording segment key (e.g., "143022_300") when token usage is attributable to a specific observation window

Usage fields (all optional depending on model capabilities):
- `input_tokens` â€“ Tokens in the prompt/input
- `output_tokens` â€“ Tokens in the response/output
- `total_tokens` â€“ Total tokens consumed
- `cached_tokens` â€“ Tokens served from cache (reduces cost)
- `reasoning_tokens` â€“ Tokens used for extended thinking/reasoning
- `requests` â€“ Number of API requests made (for batch operations)

The logging system normalizes provider-specific formats (OpenAI, Gemini, Anthropic) into this unified schema for consistent cost tracking and analysis across all models.

## Agent Event Logs

The `agents/` directory stores event logs for all AI agent sessions managed by Cortex. Each agent session produces a JSONL file containing the complete event history.

**File naming:**
- `<agent_id>_active.jsonl` â€“ currently running agent (renamed when complete)
- `<agent_id>.jsonl` â€“ completed agent session

The `agent_id` is a Unix timestamp in milliseconds that uniquely identifies the session.

**Event format (JSONL):**

Each line is a JSON object with an `event` field indicating the event type:

```jsonl
{"event": "start", "ts": 1755450767962, "persona": "helper", "prompt": "Help me with...", "facet": "work"}
{"event": "text", "ts": 1755450768000, "content": "I'll help you with that."}
{"event": "tool_call", "ts": 1755450769000, "tool": "search", "params": {"query": "example"}}
{"event": "tool_result", "ts": 1755450770000, "tool": "search", "result": "..."}
{"event": "finish", "ts": 1755450771000, "result": "Here's what I found..."}
```

**Common event types:**
- `start` â€“ agent session started, includes persona, prompt, and facet
- `text` â€“ streaming text output from the agent
- `tool_call` â€“ agent invoked an MCP tool
- `tool_result` â€“ result returned from tool execution
- `handoff` â€“ agent delegated to another agent
- `error` â€“ error occurred during execution
- `finish` â€“ agent session completed, includes final result

See [CORTEX.md](CORTEX.md) for agent architecture and spawning details.

## App Storage

The `apps/` directory provides storage space for Convey apps to persist configuration, data, and artifacts specific to this journal. Each app has its own directory at `apps/<app_name>/` where it can maintain app-specific state independent of the application codebase.

Apps typically use `config.json` for journal-specific settings and create subdirectories for data storage (e.g., `cache/`, `data/`, `logs/`). This is distinct from the app metadata file (`apps/<app>/app.json` in the codebase) which defines icon, label, and facet support across all journals. See [APPS.md](APPS.md) for storage utilities (`get_app_storage_path`, `load_app_config`, `save_app_config`).

### Chat App Storage

The chat app uses `apps/chat/chats/` to store metadata for all conversations, including both interactive agent chats and synthetic agent messages (alerts/notifications from background processes).

**Chat metadata** (`apps/chat/chats/<agent_id>.json`):
```json
{
  "agent_id": "1755450767962",
  "ts": 1755450767962,
  "facet": "work",
  "title": "Quick Math Question",
  "unread": true
}
```

Optional fields:
- `from` â€“ Sender info for synthetic agents: `{"type": "agent", "id": "mcp_tool"}`
- `unread` â€“ Boolean flag for unread status (used for badge counts)
- `archived` â€“ Boolean flag for archived status

**Synthetic agents** are created by background processes using `muse.cortex_client.create_synthetic_agent()`. These appear as completed agents in the chat interface but only contain a single `finish` event:

```jsonl
{"event": "finish", "result": "Message content in markdown format", "ts": 1755450767962}
```

This unified approach allows users to reply to any message via the chat continue feature, treating messages and agent outputs identically in the UI.

## Search Index

The `indexer/` directory contains the full-text search index built from journal content.

**Files:**
- `indexer/journal.sqlite` â€“ FTS5 SQLite database containing indexed chunks from all formattable content (insights, transcripts, events, entities, todos)

The indexer converts all content to markdown chunks via the formatters framework, then indexes with metadata fields (day, facet, topic) for filtering. Use `get_journal_index()` from `think/indexer/journal.py` to access the database programmatically.

Run `sol indexer` to rebuild the index from current journal content.

## Service Health

The `health/` directory contains log files for long-running services.

**Files:**
- `health/<service>.log` â€“ log output for each service (e.g., `observe.log`, `cortex.log`, `convey.log`)

These logs are useful for debugging service issues. See [DOCTOR.md](DOCTOR.md) for diagnostics and troubleshooting guidance.

## Imported Audio

The `imports/` directory stores audio files imported via the import app, along with their processing artifacts. Each import is organized by detected timestamp:

```
imports/
  â””â”€â”€ YYYYMMDD_HHMMSS/           # Import directory (detected or user-specified timestamp)
      â”œâ”€â”€ import.json            # Import metadata and processing status
      â”œâ”€â”€ {original_filename}    # Original uploaded audio file
      â”œâ”€â”€ imported.json          # Processed transcript in standard format
      â”œâ”€â”€ segments.json          # List of segment keys created for this import
      â””â”€â”€ summary.md             # AI-generated summary of the audio content
```

### Import metadata

The `import.json` file tracks the import process:

```json
{
  "original_filename": "meeting_recording.m4a",
  "upload_timestamp": 1755034698276,
  "upload_datetime": "2025-08-12T15:38:18.276000",
  "detection_result": {
    "day": "20250630",
    "time": "143256",
    "confidence": "high",
    "source": "Date/Time Original"
  },
  "detected_timestamp": "20250630_143256",
  "user_timestamp": "20250630_143256",
  "file_size": 13950943,
  "mime_type": "audio/x-m4a",
  "facet": "work",
  "processing_completed": "2025-08-12T15:41:42.970189"
}
```

Once processed, imports are linked into the appropriate day's segment via `imported_audio.jsonl` files that reference the original import location.

## Day folder contents

Within each day, captured content is organized into **segments** (timestamped duration folders). The folder name is the **segment key**, which uniquely identifies the segment within the day and follows this format:

- `HHMMSS_LEN/` â€“ Start time and duration in seconds (e.g., `143022_300/` for a 5-minute segment starting at 14:30:22)

Each segment progresses through the three-layer pipeline: captures are recorded, extracts are generated, and insights are synthesized.

### Layer 1: Captures

Captures are the original binary media files recorded by observation tools.

#### Audio captures

Audio files are initially written to the day root with the segment key prefix (Linux) or directly to segment folders (macOS):

- **Linux**: `HHMMSS_LEN_*.flac` â€“ audio files in day root (e.g., `143022_300_audio.flac`)
- **macOS**: `HHMMSS_LEN/audio.m4a` â€“ audio files written directly to segment folder

After transcription, audio files are moved into their segment folder:

- `HHMMSS_LEN/*.flac` or `*.m4a` â€“ audio files moved here after processing, preserving descriptive suffix (e.g., `audio.flac`, `audio.m4a`, `mic.flac`)

Note: The descriptive portion after the segment key (e.g., `_audio`, `_recording`) is preserved when files are moved into segment directories. Processing tools match files by extension only, ignoring the descriptive suffix.

#### Screen captures

Screen recordings use per-monitor files with position and connector/displayID in the filename:

- **Linux**: `HHMMSS_LEN_<position>_<connector>_screen.webm` â€“ screencast video files in day root (e.g., `143022_300_center_DP-3_screen.webm`)
- **macOS**: `HHMMSS_LEN/<position>_<displayID>_screen.mov` â€“ video files written directly to segment folder (e.g., `center_1_screen.mov`)

After analysis, files are in their segment folder:

- `HHMMSS_LEN/<position>_<connector>_screen.webm` or `*.mov` â€“ video files (e.g., `center_DP-3_screen.webm`, `center_1_screen.mov`)

For multi-monitor setups, each monitor produces a separate file. Position labels include: `center`, `left`, `right`, `top`, `bottom`, and combinations like `left-top`.

### Layer 2: Extracts

Extracts are structured data files (JSON/JSONL) derived from captures through AI analysis.

#### Audio transcript extracts

The transcript file (`audio.jsonl`) contains a metadata line followed by one JSON object per transcript segment.

Example transcript file:

```jsonl
{"raw": "audio.flac"}
{"start": "00:00:01", "source": "mic", "text": "So we need to finalize the authentication module today."}
{"start": "00:00:15", "source": "sys", "text": "I agree. Let's make sure we have proper unit tests."}
```

**Metadata line (first line):**
- `raw` â€“ path to processed audio file (required)
- `backend` â€“ STT backend used (e.g., "whisper", "revai")
- `model` â€“ model used for transcription (e.g., "medium.en", "revai-fusion")
- `device` â€“ device used for inference (e.g., "cuda", "cpu", "cloud")
- `compute_type` â€“ compute precision used (e.g., "float16", "int8", "api")
- `remote` â€“ remote name if transcribed from a remote source (optional)
- `imported` â€“ object with import metadata for external files (optional):
  - `id` â€“ unique import identifier
  - `facet` â€“ facet name for entity extraction
  - `setting` â€“ contextual setting description

**Transcript statements (subsequent lines):**
- `start` â€“ timestamp in HH:MM:SS format (required)
- `text` â€“ transcribed text (required)
- `source` â€“ audio source: "mic" or "sys" (optional)
- `speaker` â€“ speaker identifier, numeric or string (optional, not currently populated)
- `corrected` â€“ LLM-corrected version of text (optional, added during enrichment)
- `description` â€“ tone or delivery description, e.g., "enthusiastic", "questioning" (optional, added during enrichment)

#### Screen frame extracts

Screen analysis files use per-monitor naming: `<position>_<connector>_screen.jsonl` (e.g., `center_DP-3_screen.jsonl`, `left_HDMI-1_screen.jsonl`). For single-monitor setups, the file is simply `screen.jsonl`. Each file contains one JSON object per qualified frame. Frames qualify when they show significant visual change (â‰¥5% RMS difference) compared to the previous qualified frame.

Example frame record:

```json
{
  "frame_id": 123,
  "timestamp": 45.67,
  "requests": [
    {"type": "describe", "model": "gemini-2.5-flash-lite", "duration": 0.5},
    {"type": "category", "category": "reading", "model": "gemini-3-flash", "duration": 1.2}
  ],
  "analysis": {
    "visual_description": "Documentation page showing API reference.",
    "primary": "reading",
    "secondary": "none",
    "overlap": true
  },
  "content": {
    "reading": "# API Reference\n\n## Authentication\n\nUse Bearer tokens..."
  }
}
```

**Common fields:**
- `frame_id` â€“ sequential frame number in the video
- `timestamp` â€“ time in seconds from video start
- `requests` â€“ list of vision API requests made for this frame (type: "describe" for initial, "category" for follow-ups)
- `analysis` â€“ categorization result with `primary`, `secondary`, `overlap`, and `visual_description`
- `content` â€“ object containing category-specific extracted content (see below)
- `error` â€“ present when processing failed after retries

**Category-specific content (inside `content` object):**
- `messaging` â€“ markdown content when frame contains chat/email apps
- `browsing` â€“ markdown content when frame contains web browsing
- `reading` â€“ markdown content when frame contains documents/articles
- `productivity` â€“ markdown content when frame contains spreadsheets/slides/calendars
- `meeting` â€“ JSON object when frame contains video conferencing, includes participant detection and bounding boxes

The vision analysis uses multi-stage conditional processing:
1. Initial categorization determines content type (e.g., `code`, `meeting`, `browsing`, `reading`). See `observe/categories/` for the full list of categories.
2. Category-specific follow-up prompts are discovered from `observe/categories/*.txt` files
3. Follow-ups are triggered for categories that have a `.txt` template (currently: messaging, browsing, reading, productivity output markdown; meeting outputs JSON)

#### Event extracts

Insight generation extracts time-based events from the day's transcriptsâ€”meetings, messages, follow-ups, file activity and more. Events are stored per-facet in JSONL files at `facets/{facet}/events/{day}.jsonl`.

There are two types of events:
- **Occurrences** â€“ events that happened on the capture day (`occurred: true`)
- **Anticipations** â€“ future scheduled events extracted from calendar views (`occurred: false`)

```jsonl
{"type": "meeting", "start": "09:00:00", "end": "09:30:00", "title": "Team stand-up", "summary": "Status update with the engineering team", "work": true, "participants": ["Jeremie Miller", "Alice", "Bob"], "facet": "work", "topic": "meetings", "occurred": true, "source": "20250101/insights/meetings.md", "details": "Sprint planning discussion"}
{"type": "deadline", "date": "2025-01-15", "start": null, "end": null, "title": "Project milestone", "summary": "Q1 deliverable due", "work": true, "participants": [], "facet": "work", "topic": "schedule", "occurred": false, "source": "20250101/insights/schedule.md", "details": "Final review before release"}
```

**Common fields:**
- **type** â€“ event kind: `meeting`, `message`, `file`, `followup`, `documentation`, `research`, `media`, `deadline`, `appointment`, etc.
- **start** and **end** â€“ HH:MM:SS timestamps (or `null` for anticipations without specific times)
- **date** â€“ ISO date YYYY-MM-DD (anticipations only, indicates scheduled date)
- **title** and **summary** â€“ short text for display and search
- **facet** â€“ facet name the event belongs to (required)
- **topic** â€“ source insight type (e.g., "meetings", "schedule", "flow")
- **occurred** â€“ `true` for occurrences, `false` for anticipations
- **source** â€“ path to the insight file that generated this event
- **work** â€“ boolean, work vs. personal classification
- **participants** â€“ optional list of people or entities involved
- **details** â€“ free-form string with additional context

This structure allows the indexer to collect and search events across all facets and days.

### Layer 3: Insights

Insights are AI-generated markdown files that provide human-readable narratives synthesized from captures and extracts.

#### Segment insights

After captures are processed, segment-level insights are generated within each segment folder as `HHMMSS_LEN/*.md` files. Available segment insight types are defined by templates in `think/insights/` with `"frequency": "segment"` in their metadata JSON.

#### Daily insights

Post-processing generates day-level insights in the `insights/` directory that synthesize all segments.

**Insight discovery:** Available insight types are discovered at runtime from:
- `think/insights/*.txt` â€“ system insight templates
- `apps/{app}/insights/*.txt` â€“ app-specific insight templates

Each template has a companion `.json` file with metadata (title, description, frequency, output format). Use `get_insights()` from `think/utils.py` to retrieve all available insights programmatically.

**Output naming:**
- System insights: `insights/{topic}.md` (e.g., `insights/flow.md`, `insights/meetings.md`)
- App insights: `insights/_{app}_{topic}.md` (e.g., `insights/_chat_sentiment.md`)
- JSON output: `insights/{topic}.json` when metadata specifies `"output": "json"`

Each insight type has a corresponding template file (`{name}.txt`) that defines how the AI synthesizes extracts into narrative form.
